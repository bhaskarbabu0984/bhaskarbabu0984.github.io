
# Introduction

This module introduces cloud architecture principles. To take advantage of a cloud infrastructure for your business, while complying with the AWS Well-Architected Framework, you should know these principles.

Applying these cloud principles will help you to build highly available, scalable architectures within AWS. The cloud principles are as follows:

- Designing for failure
- Decoupled components instead of monolithic architecture
- Implementing elasticity in the cloud as opposed to on-premises environments
- Thinking parallel

We illustrate the cloud architecture design principles with a few examples.

### Designing for failure

An example of designing for failure is the use of a load balancer. Elastic Load Balancing (ELB) is a managed load balancing service that automatically distributes incoming application traffic across multiple targets. Examples of such targets include Amazon Elastic Compute Cloud (Amazon EC2) instances, containers, IP addresses, and AWS Lambda functions. A load balancer only sends traffic to healthy targets. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones.

![Cloud_Architectural_Principles_ELB_thick_NOPROCESS_](https://github.com/user-attachments/assets/d16da7f0-d2cc-44e4-be61-eeb4796b2be1)
<p align="center"> <i>An ELB load balancer distributes traffic between EC2 instances in two Availability Zones. </i> </p>


**Example of a load balancer**

In this example, the ELB load balancer distributes incoming requests between the Amazon EC2 instances:

If an instance is unhealthy, the load balancer redirects traffic to the other instance.
The load balancer works with Amazon EC2 Auto Scaling to ensure that the right amount of instances are available at any time. The load balancer also helps to ensure that no instance is overwhelmed.

### Loosely coupling
Traditional infrastructures have chains of tightly integrated components. Each component has a specific purpose.

Components are strongly connected to each other, which impedes scaling and makes the system more likely to experience failures.

With loose coupling, you reduce dependencies in your system by using managed solutions as intermediaries between the layers of your system. In this way, the intermediary automatically handles failures and the scaling of components or layers.

One way to achieve loose coupling is by using asynchronous messages that use Amazon Simple Queue Service (Amazon SQS), for example. Separating processes into different pieces that are connected by messages in queues creates clear transaction boundaries and allows services to operate more independently.

Consider the following diagram that shows an example of a queue-based architecture resulting in a loosely coupled system.
![Cloud_Architectural_Principles_Loosely_Coupled_NOPROCESS_](https://github.com/user-attachments/assets/ebfc92cd-8732-47ab-b643-ef0676779d93)
<p align="center"> <i>Queue-based architecture using Amazon SQS.</i> </p>

1. Multiple users submit jobs with the AWS Command Line Interface (AWS CLI) or AWS SDKs.
2. The jobs are queued as messages in Amazon SQS.
3. EC2 instances poll the queue and start processing jobs.
4. Amazon SQS emits metrics based on the number of messages (jobs) in the queue.
5. An Amazon CloudWatch alarm is configured to notify Amazon EC2 Auto Scaling if the queue is longer than a specified length. Amazon EC2 Auto Scaling increases the number of EC2 instances.
6. The EC2 instances pull source data and store result data in an Amazon Simple Storage Service (Amazon S3) bucket.

In this architecture, Amazon SQS decouples the users' requests and the EC2 instances to avoid bottlenecks. Imagine that the request is to upload images, process them, create thumbnails, and upload them to a storage service such as Amazon S3. The EC2 instances can manage a limited number of requests at a time. One of the solutions is to send messages to the queue so that EC2 instances fetch messages and process them when they have finished processing the previous requests.

### Embracing elasticity and automating
A common cause of failure in on-premises workloads is resource saturation, or when the demands placed on a workload exceed the capacity of that workload (this failure is often the objective of denial-of-service attacks). In the cloud, you can monitor demand and workload utilization (by using Amazon CloudWatch, for example). You can automate the addition or removal of resources to maintain the optimal level to satisfy demand without overprovisioning or underprovisioning.

An example of automation that leads to elasticity is the use of an Auto Scaling group to scale in or scale out EC2 instances.

![Auto_Scaling_Group_NOPROCESS_](https://github.com/user-attachments/assets/7b28faec-7ca0-4b67-935e-58e19bbd65bf)

<p align="center"> <i>Placing your EC2 instances in Auto Scaling groups helps you to adapt the number of instances to your needs.</i> </p>

### Using parallelism
Parallelism means breaking down larger tasks into smaller, yet similar, subtasks. These subtasks are processed independently. The respective results are then combined at the end as a single process, which allows for increased efficiencies.

There are different types of parallelism. Data parallelism means multiple instances of the same task operating on different sections of data. Task parallelism means different tasks operating on the same or different data.

![Cloud_Architectural_Principles_Parallelism_thick_NO_PROCESS_](https://github.com/user-attachments/assets/f6e4b5cd-52ea-4e13-85de-ca2dda87971c)
<p align="center"> <i>Placing your EC2 instances in Auto Scaling groups helps you to adapt the number of instances to your needs.</i> </p>

Elastic Load Balancing ensures that the traffic is evenly distributed across instances so that requests are handled in parallel.
